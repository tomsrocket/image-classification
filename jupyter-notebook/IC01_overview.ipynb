{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0aee67d1-2c64-4908-8356-1b863e0a8f1d"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image classification with Python\n",
    "..or how I won a hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "915f7875-a094-432d-b889-8a385bb1d11e"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About me\n",
    "\n",
    "Thomas Werner\n",
    "    http://twitter.com/toms_rocket\n",
    "\n",
    "* Software Craftsman (Check out the local Software Craftsmanship Community https://www.softwerkskammer.org/groups/socramob)\n",
    "* Local Open Data Group / OK Lab MÃ¼nster http://codeformuenster.org\n",
    "* Github: https://github.com/tomsrocket (code & slides)\n",
    "* Not an expert on the topic of image classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image Classification\n",
    "\n",
    "Let a computer solve the question: \n",
    "* \"Is this a day or night photo?\"\n",
    "* \"Is this a company logo?\"\n",
    "* \"Does this picture show an animal?\"\n",
    "* \"Which animal is on this picture\"\n",
    "\n",
    "How is this done?\n",
    "* Best choice right now would be to use Deep Convolutional Neural Networks (CNN)\n",
    "  * All recent ImageNet Challenge (ILSVRC) winners used CNNs\n",
    "    * Yearly challenge from the standford university\n",
    "    * All the big players participating: Google, Microsoft, ..\n",
    "    * Results 2015: http://image-net.org/challenges/LSVRC/2015/results\n",
    "  * A great explanation of how CNNs work is here: \n",
    "  http://cs231n.github.io/convolutional-networks/ (Andrej Karpathy)\n",
    "\n",
    "\n",
    "* We will be using a support vector machine (SVM)\n",
    "  * SVMs were state of the art for image classification problems before CNN came up\n",
    "  * Relatively easy to understand  \n",
    "  * Easy to use\n",
    "  * Fast\n",
    "  * Very good results for the chosen classification problem\n",
    "  * You have to define the feature vector manually\n",
    "\n",
    "* Experiments with combining SVM / CNN have also been done: http://deeplearning.net/wp-content/uploads/2013/03/dlsvm.pdf\n",
    "\n",
    "\n",
    "## Scikit-learn - \"Machine Learning in Python\"\n",
    "\n",
    "[Introduction to the scikit-learn library](IC02_intro.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Support vector machine\n",
    "\n",
    "http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "From a programmers point of view: \n",
    "* Define the classes you want to divide the images into\n",
    "* You need a LOT of images from each of the classes\n",
    "* You define a suitable feature vector\n",
    "* You calculate feature vectors for all your images\n",
    "* You feed the feature vectors into the SVM\n",
    "* You tell the SVM which class each feature vector belongs to\n",
    "* You let the SVM train on the images\n",
    "* You get a \"trained SVM\" = we call that \"classifier\"\n",
    "* Now you can feed an \"unknown\"/unclassified image into the classifier, and it will tell you which class it belongs to \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Feature Vector\n",
    "\n",
    "*Very important, the feature vectors will be the only thing the SVM gets, and the SVM needs to be able to distinguish the images by it*\n",
    "\n",
    "* I chose a feature vector that counts the \"number of pixels per color\"\n",
    "* Implementation details:\n",
    "  * Problem 1: \n",
    "    * Just counting the number of pixels for each color = feature vectors wouldnt be comparable, but dependent on the size of each image\n",
    "    * Solution: Feature vector will contain the ratio of pixels of each color by the total number of pixels (=normalized)\n",
    "  * Problem 2: \n",
    "    * RGB color space = 16M colors, feature vector too large\n",
    "    * Solution: map each of the 256 possible values of each color channel to a smaller set of values, for example 4\n",
    "  * Problem 3: \n",
    "    * Feature vector is expected to be 1-dimensional\n",
    "    * Solution: combine the color values into 1 dimension \n",
    "* Detailled description: http://www.ippatsuman.com/2014/08/13/day-and-night-an-image-classifier-with-scikit-learn/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets see some code!\n",
    "\n",
    "### How to calculate the feature vector\n",
    "* [The image class that we want to match](IC03b_feature_vector_logos.ipynb)\n",
    "* [The other images](IC03c_feature_vector_images.ipynb)\n",
    "\n",
    "Lots of different feature vectors are imaginable, e.g. scale every image down to 10x10pixels, use that as feature vector\n",
    "\n",
    "\n",
    "## Implementation: Main Method\n",
    "\n",
    "[More code ahead](IC04_svm_version1.ipynb)\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "That's it, thanks for listening!\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {
    "2eb02082-a9a0-4a25-8936-09a9bd37a57d": {
     "id": "2eb02082-a9a0-4a25-8936-09a9bd37a57d",
     "prev": "46039fee-f2cf-42d7-a136-1c79c3562ac5",
     "regions": {
      "75242b4c-423d-42e9-b714-8389bb9805e1": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "915f7875-a094-432d-b889-8a385bb1d11e",
        "part": "whole"
       },
       "id": "75242b4c-423d-42e9-b714-8389bb9805e1"
      }
     }
    },
    "46039fee-f2cf-42d7-a136-1c79c3562ac5": {
     "id": "46039fee-f2cf-42d7-a136-1c79c3562ac5",
     "prev": "e23faa55-06bb-4c7f-afb3-957de13f0d99",
     "regions": {
      "b787148d-73d6-4d53-9d8e-46aea4b52505": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "09290caa-f4c8-44aa-96a4-80a463f5c235",
        "part": "whole"
       },
       "id": "b787148d-73d6-4d53-9d8e-46aea4b52505"
      }
     }
    },
    "e23faa55-06bb-4c7f-afb3-957de13f0d99": {
     "id": "e23faa55-06bb-4c7f-afb3-957de13f0d99",
     "prev": null,
     "regions": {
      "51dc2189-2397-48c0-b780-0d19d5623a9a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "0aee67d1-2c64-4908-8356-1b863e0a8f1d",
        "part": "whole"
       },
       "id": "51dc2189-2397-48c0-b780-0d19d5623a9a"
      }
     }
    }
   },
   "themes": {
    "default": "502b1140-02af-4362-8b66-e0d2ef29fc72",
    "theme": {
     "502b1140-02af-4362-8b66-e0d2ef29fc72": {
      "id": "502b1140-02af-4362-8b66-e0d2ef29fc72",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         155,
         177,
         192
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410"
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 8
       },
       "h2": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "font-family": "Merriweather",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
